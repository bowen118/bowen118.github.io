---
---

@inproceedings{lu2023visual,
  title={Visual Language Pretrained Multiple Instance Zero-Shot Transfer for Histopathology Images},
  author={Lu, Ming Y and Chen, Bowen and Zhang, Andrew and Williamson, Drew FK and Chen, Richard J and Ding, Tong and Le, Long Phi and Chuang, Yung-Sung and Mahmood, Faisal},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={19764--19775},
  year={2023},
  preview={mizero_tall.jpg},
  abstract={Contrastive visual language pretraining has emerged as a powerful method for either training new language-aware image encoders or augmenting existing pretrained models with zero-shot visual recognition capabilities. However, existing works typically train on large datasets of image-text pairs and have been designed to perform downstream tasks involving only small to medium sized-images, neither of which are applicable to the emerging field of computational pathology where there are limited publicly available paired image-text datasets and each image can span up to 100,000 x 100,000 pixels in dimensions. In this paper we present MI-Zero, a simple and intuitive framework for unleashing the zero-shot transfer capabilities of contrastively aligned image and text models to gigapixel histopathology whole slide images, enabling multiple downstream diagnostic tasks to be carried out by pretrained encoders without requiring any additional labels. MI-Zero reformulates zero-shot transfer under the framework of multiple instance learning to overcome the computational challenge of inference on extremely large images. We used over 550k pathology reports and other available in-domain text corpora to pretrain our text encoder. By effectively leveraging strong pretrained encoders, our best model pretrained on over 33k histopathology image-caption pairs achieves an average median zero-shot accuracy of 70.2% across three different real-world cancer subtyping tasks. Our code is available at: https://github.com/mahmoodlab/MI-Zero.}
}

@article{lipkova2022artificial,
  bibtex_show={true},
  selected={true},
  html={https://www.sciencedirect.com/science/article/pii/S153561082200441X},
  title={Artificial intelligence for multimodal data integration in oncology},
  author={Lipkova, Jana and Chen, Richard J and Chen, Bowen and Lu, Ming Y and Barbieri, Matteo and Shao, Daniel and Vaidya, Anurag J and Chen, Chengkuan and Zhuang, Luoting and Williamson, Drew FK and others},
  journal={Cancer Cell},
  volume={40},
  number={10},
  pages={1095--1110},
  year={2022},
  publisher={Elsevier},
  preview={multimodal_review.jpg},
  abstract={In oncology, the patient state is characterized by a whole spectrum of modalities, ranging from radiology, histology, and genomics to electronic health records. Current artificial intelligence (AI) models operate mainly in the realm of a single modality, neglecting the broader clinical context, which inevitably diminishes their potential. Integration of different data modalities provides opportunities to increase robustness and accuracy of diagnostic and prognostic models, bringing AI closer to clinical practice. AI models are also capable of discovering novel patterns within and across modalities suitable for explaining differences in patient outcomes or treatment resistance. The insights gleaned from such models can guide exploration studies and contribute to the discovery of novel biomarkers and therapeutic targets. To support these advances, here we present a synopsis of AI methods and strategies for multimodal data fusion and association discovery. We outline approaches for AI interpretability and directions for AI-driven exploration through multimodal data interconnections. We examine challenges in clinical adoption and discuss emerging solutions.},
}

@article{chen2021abstract,
  bibtex_show={true},
  selected={true},
  html={https://aacrjournals.org/clincancerres/article/27/5_Supplement/PR-01/32755/Abstract-PR-01-Real-time-point-of-care-pathology},
  title={Abstract PR-01: Real-time, point-of-care pathology diagnosis via embedded deep learning},
  author={Chen, Bowen and Lu, Max and Lipkova, Jana and Mahmood, Faisal},
  journal={Clinical Cancer Research},
  volume={27},
  number={5\_Supplement},
  pages={PR--01},
  year={2021},
  publisher={AACR},
  preview={aacr_talk.png},
  abstract={There is an urgent need for widespread cancer diagnosis in low resource settings, especially in contrast to areas with developed healthcare systems. According to a study in The Lancet, in the U.S. there is one pathologist for every 20,000 individuals, while in Sub-Saharan Africa, there is only one for every million. In addition, current telepathology systems for cancer diagnosis mostly rely on pathologists performing remotely, which is low-throughput and requires more time and resources. With the growth of telepathology, remote diagnosis becomes a viable solution to address the lack of skilled pathologists in developing regions. Here, we present a cost-efficient device that incorporates embedded deep learning to achieve real time, point-of-care diagnosis of whole pathology slides. We achieve this with a low-cost, 3D-printable microscope that uses the Raspberry Pi and camera module to capture high-resolution images of slides. Then, using a weakly-supervised deep-learning model run on the NVIDIA Jetson Nano, the device is able to accurately classify the whole slide without any pixel-level annotations. Furthermore, the model’s attention-based approach to diagnosis allows us to generate human-interpretable heatmaps displaying the regions most influential to the model’s diagnosis. Our device also incorporates a touch screen and batteries to increase accessibility as an easy-to-use and low maintenance device while still maintaining an efficient runtime given the available resources. Overall, we demonstrate that the device is capable of achieving accurate, high-throughput, and interpretable cancer diagnoses in low resource settings.}
}
